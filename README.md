# PowerFlow : An Integrated Data Pipeline for Electricity Analytics
Le projet porte sur l’analyse et l’exploitation de données d’électricité pour des objectifs tels que la compréhension des tendances, la prévision de la consommation ou l’identification d’anomalies.
Description
PowerFlow est un projet complet couvrant toutes les étapes d'un pipeline data autour des données d'électricité. Il inclut :

Data ingénierie : Extraction, transformation et chargement (ETL) des données.
DevOps : Intégration continue, tests unitaires et déploiement sur Kubernetes.
Data science : Modèle de machine learning pour prédire la consommation électrique.
Data analysis : Visualisation des données et analyse des tendances.
Ce projet est conteneurisé avec Docker et conçu pour être facilement déployé sur Kubernetes.
Fonctionnalités
ETL (Data Engineering) :

Ingestion de données (fichiers CSV, API ou bases de données).
Transformation des données : nettoyage, enrichissement et normalisation.
Stockage des données nettoyées sur Google Cloud Storage ou Bitbucket.
DevOps :

Tests unitaires avec pytest.
Analyse de code avec pylint et formatage avec black.
Pipeline CI/CD via GitHub Actions pour automatiser les tests, l'analyse, et le déploiement.
Data Science :

Modèle prédictif pour la consommation électrique basé sur des données historiques.
Évaluation des performances du modèle avec des métriques standards.
Data Analysis :

Visualisation interactive des données avec matplotlib, seaborn, et plotly.
Rapports d’analyse des tendances et insights clés.
Fonctionnalités
ETL (Data Engineering) :

Ingestion de données (fichiers CSV, API ou bases de données).
Transformation des données : nettoyage, enrichissement et normalisation.
Stockage des données nettoyées sur Google Cloud Storage ou Bitbucket.
DevOps :

Tests unitaires avec pytest.
Analyse de code avec pylint et formatage avec black.
Pipeline CI/CD via GitHub Actions pour automatiser les tests, l'analyse, et le déploiement.
Data Science :

Modèle prédictif pour la consommation électrique basé sur des données historiques.
Évaluation des performances du modèle avec des métriques standards.
Data Analysis :

Visualisation interactive des données avec matplotlib, seaborn, et plotly.
Rapports d’analyse des tendances et insights clés.
